{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_test_batches = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 11 - Secure Deep Learning Classification \n",
    "\n",
    "\n",
    "\n",
    "## Your data matters, your model too\n",
    "\n",
    "Data is the driver behind Machine Learning. Organizations who create and collect data are able to build and train their own machine learning models. This allows them to offer the use of such models as a service (MLaaS) to outside organizations. This is useful as other organizations who might not be able to create these models themselves but who still would like to use this model to make predictions on their own data. \n",
    "\n",
    "However, a model hosted in the cloud still presents a privacy/IP issue. In order for external organizations to use it - they must either upload their input data (such as images to be classified) or download the model. Uploading input data can be problematic from a privacy perspective, but downloading the model might not be an option if the organization who created/owns the model is worried about losing their IP.\n",
    "\n",
    "\n",
    "## Computing over encrypted data\n",
    "\n",
    "In this context, one potential solution is to encrypt both the model and the data in a way which allows one organization to use a model owned by another organization without either disclosing their IP to one another. Several encryption schemes exist that allow for computation over encrypted data, among which Secure Multi-Party Computation (SMPC), Homomorphic Encryption (FHE/SHE) and Functional Encryption (FE) are the most well known types. We will focus here on Secure Multi-Party Computation ([introduced in detail here in tutorial 5](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%205%20-%20Intro%20to%20Encrypted%20Programs.ipynb)) which consists of private additive sharing. It relies on crypto protocols such as SecureNN and SPDZ, the details of which are given [in this excellent blog post](https://mortendahl.github.io/2017/09/19/private-image-analysis-with-mpc/). \n",
    "\n",
    "These protocols achieve remarkable performances over encrypted data, and over the past few months we have been working to make these protocols easy to use. Specifically, we're building tools to allow you to use these protocols without having to re-implement the protocol yourself (or even necessarily know the cryptography behind how it works). Let's jump right in.\n",
    "\n",
    "## Set up\n",
    "\n",
    "The exact setting in this tutorial is the following: consider that you are the server and you have some data. First, you define and train a model with this private training data. Then, you get in touch with a client who holds some of their own data who would like to access your model to make some predictions. \n",
    "\n",
    "You encrypt your model (a neural network). The client encrypts their data. You both then use these two encrypted assets to use the model to classify the data. Finally, the result of the prediction is sent back to the client in an encrypted way so that the server (_i.e._ you) learns nothing about the client's data (you learn neither the inputs or the prediction).\n",
    "\n",
    "Ideally we would additively share the `client`'s input between itself and the `server` and vice versa for the model. For the sake of simplicity, the shares will be held by two other workers `alice` and `bob`. If you consider that alice is owned by the client and bob by the server, it's completely equivalent.\n",
    "\n",
    "The computation is secure in the honest-but-curious adversary model which is standard in [many MPC frameworks](https://arxiv.org/pdf/1801.03239.pdf).\n",
    "\n",
    "**We have now everything we need, let's get started!**\n",
    "\n",
    "\n",
    "Author:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   We also need to execute commands specific to importing/starting PySyft. We create a few workers (named `client`, `bob`, and `alice`). Lastly, we define the `crypto_provider` who gives all the crypto primitives we may need ([See our tutorial on SMPC for more details](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2009%20-%20Intro%20to%20Encrypted%20Programs.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'F:\\Anaconda3\\envs\\tfe\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.2.so'\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch) \n",
    "client = sy.VirtualWorker(hook, id=\"client\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the setting of the learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 50\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 100\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and sending to workers\n",
    "\n",
    "In our setting, we assume that the server has access to some data to first train its model. Here is the MNIST training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the client has some data and would like to have predictions on it using the server's model. This client encrypts its data by sharing it additively across two workers `alice` and `bob`.\n",
    "> SMPC uses crypto protocols which require to work on integers. We leverage here the PySyft tensor abstraction to convert PyTorch Float tensors into Fixed Precision Tensors using `.fix_precision()`. For example 0.123 with precision 2 does a rounding at the 2nd decimal digit so the number stored is the integer 12. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "        data.fix_precision().share(alice, bob, crypto_provider=crypto_provider),\n",
    "        target.fix_precision().share(alice, bob, crypto_provider=crypto_provider)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network specification\n",
    "Here is the network specification used by the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training\n",
    "The training is done locally so this is pure local PyTorch training, nothing special here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.345264\tTime: 0.919s\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 0.339435\tTime: 0.015s\n",
      "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.190739\tTime: 0.011s\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.271642\tTime: 0.011s\n",
      "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.168248\tTime: 0.012s\n",
      "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.281684\tTime: 0.010s\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.231250\tTime: 0.011s\n",
      "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.162955\tTime: 0.013s\n",
      "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.139782\tTime: 0.010s\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.019391\tTime: 0.010s\n",
      "\n",
      "Test set: Average loss: 0.1000, Accuracy: 9697/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.113673\tTime: 0.011s\n",
      "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.175307\tTime: 0.011s\n",
      "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.192445\tTime: 0.010s\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.041772\tTime: 0.012s\n",
      "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.089853\tTime: 0.010s\n",
      "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.102471\tTime: 0.010s\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.273505\tTime: 0.010s\n",
      "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.031482\tTime: 0.012s\n",
      "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.115033\tTime: 0.011s\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.130946\tTime: 0.012s\n",
      "\n",
      "Test set: Average loss: 0.0757, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.048327\tTime: 0.031s\n",
      "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.069229\tTime: 0.023s\n",
      "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.035326\tTime: 0.013s\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.028025\tTime: 0.012s\n",
      "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.010495\tTime: 0.010s\n",
      "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.032783\tTime: 0.019s\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.053168\tTime: 0.010s\n",
      "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.154932\tTime: 0.010s\n",
      "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.012133\tTime: 0.013s\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.013864\tTime: 0.009s\n",
      "\n",
      "Test set: Average loss: 0.0724, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.016712\tTime: 0.025s\n",
      "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.119643\tTime: 0.017s\n",
      "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.003078\tTime: 0.014s\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.013845\tTime: 0.010s\n",
      "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.020984\tTime: 0.010s\n",
      "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.025241\tTime: 0.013s\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.005022\tTime: 0.010s\n",
      "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.092998\tTime: 0.013s\n",
      "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.034648\tTime: 0.009s\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.070210\tTime: 0.018s\n",
      "\n",
      "Test set: Average loss: 0.0707, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.108722\tTime: 0.011s\n",
      "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.014042\tTime: 0.012s\n",
      "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.005322\tTime: 0.080s\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.027733\tTime: 0.010s\n",
      "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.003713\tTime: 0.089s\n",
      "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.007381\tTime: 0.010s\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.092807\tTime: 0.009s\n",
      "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.001091\tTime: 0.011s\n",
      "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.003834\tTime: 0.016s\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.117205\tTime: 0.030s\n",
      "\n",
      "Test set: Average loss: 0.0762, Accuracy: 9787/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.042390\tTime: 0.010s\n",
      "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.007911\tTime: 0.010s\n",
      "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.001123\tTime: 0.010s\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.034604\tTime: 0.039s\n",
      "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.010097\tTime: 0.050s\n",
      "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.003753\tTime: 0.010s\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.006738\tTime: 0.017s\n",
      "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.001874\tTime: 0.013s\n",
      "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.015255\tTime: 0.012s\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.007801\tTime: 0.012s\n",
      "\n",
      "Test set: Average loss: 0.0729, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.066859\tTime: 0.010s\n",
      "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.005586\tTime: 0.010s\n",
      "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.002238\tTime: 0.010s\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.093172\tTime: 0.010s\n",
      "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.004869\tTime: 0.010s\n",
      "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.000263\tTime: 0.012s\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.013850\tTime: 0.011s\n",
      "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.009815\tTime: 0.011s\n",
      "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.107065\tTime: 0.012s\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.081768\tTime: 0.010s\n",
      "\n",
      "Test set: Average loss: 0.0774, Accuracy: 9780/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.138442\tTime: 0.012s\n",
      "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.015963\tTime: 0.014s\n",
      "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.003375\tTime: 0.012s\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.039690\tTime: 0.010s\n",
      "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.016301\tTime: 0.010s\n",
      "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.032110\tTime: 0.009s\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.051706\tTime: 0.015s\n",
      "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.017902\tTime: 0.012s\n",
      "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.064313\tTime: 0.011s\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.020518\tTime: 0.010s\n",
      "\n",
      "Test set: Average loss: 0.0813, Accuracy: 9788/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.089528\tTime: 0.010s\n",
      "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.005134\tTime: 0.014s\n",
      "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.004820\tTime: 0.010s\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.021585\tTime: 0.010s\n",
      "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.035256\tTime: 0.026s\n",
      "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.020004\tTime: 0.010s\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.010411\tTime: 0.120s\n",
      "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.028300\tTime: 0.017s\n",
      "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.009534\tTime: 0.011s\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.006694\tTime: 0.011s\n",
      "\n",
      "Test set: Average loss: 0.0881, Accuracy: 9782/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.069914\tTime: 0.009s\n",
      "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.000369\tTime: 0.010s\n",
      "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.002586\tTime: 0.010s\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.003947\tTime: 0.011s\n",
      "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.122307\tTime: 0.010s\n",
      "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.017797\tTime: 0.010s\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.011365\tTime: 0.010s\n",
      "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.001051\tTime: 0.010s\n",
      "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.047750\tTime: 0.011s\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.003785\tTime: 0.011s\n",
      "\n",
      "Test set: Average loss: 0.0724, Accuracy: 9826/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)\n",
    "    test(args, model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is now trained and ready to be provided as a service!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secure evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as the server, we send the model to the workers holding the data. Because the model is sensitive information (you've spent time optimizing it!), you don't want to disclose its weights so you secret share the model just like we did with the dataset earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test function performs the encrypted evaluation. The model weights, the data inputs, the prediction and the target used for scoring are encrypted!\n",
    "\n",
    "However, the syntax is very similar to pure PyTorch testing of a model, isn't it nice?!\n",
    "\n",
    "The only thing we decrypt from the server side is the final score at the end to verify predictions were on average good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct_priv = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader[:n_test_batches]:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1) \n",
    "            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n",
    "            n_total += args.test_batch_size\n",
    "# This 'test' function performs the encrypted evaluation. The model weights, the data inputs, the prediction and the target used for scoring are all encrypted!\n",
    "\n",
    "# However as you can observe, the syntax is very similar to normal PyTorch testing! Nice!\n",
    "\n",
    "# The only thing we decrypt from the server side is the final score at the end of our 200 items batches to verify predictions were on average good.      \n",
    "            n_correct = n_correct_priv.copy().get().float_precision().long().item()\n",
    "    \n",
    "            print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                n_correct, n_total,\n",
    "                100. * n_correct / n_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 49/50 (98%)\n",
      "Test set: Accuracy: 98/100 (98%)\n",
      "Test set: Accuracy: 146/150 (97%)\n",
      "Test set: Accuracy: 196/200 (98%)\n",
      "Test set: Accuracy: 245/250 (98%)\n",
      "Test set: Accuracy: 295/300 (98%)\n",
      "Test set: Accuracy: 345/350 (99%)\n",
      "Test set: Accuracy: 395/400 (99%)\n",
      "Test set: Accuracy: 444/450 (99%)\n",
      "Test set: Accuracy: 492/500 (98%)\n",
      "Test set: Accuracy: 541/550 (98%)\n",
      "Test set: Accuracy: 590/600 (98%)\n",
      "Test set: Accuracy: 639/650 (98%)\n",
      "Test set: Accuracy: 688/700 (98%)\n",
      "Test set: Accuracy: 737/750 (98%)\n",
      "Test set: Accuracy: 785/800 (98%)\n",
      "Test set: Accuracy: 835/850 (98%)\n",
      "Test set: Accuracy: 884/900 (98%)\n",
      "Test set: Accuracy: 933/950 (98%)\n",
      "Test set: Accuracy: 983/1000 (98%)\n",
      "Test set: Accuracy: 1033/1050 (98%)\n",
      "Test set: Accuracy: 1083/1100 (98%)\n",
      "Test set: Accuracy: 1132/1150 (98%)\n",
      "Test set: Accuracy: 1179/1200 (98%)\n",
      "Test set: Accuracy: 1229/1250 (98%)\n",
      "Test set: Accuracy: 1278/1300 (98%)\n",
      "Test set: Accuracy: 1327/1350 (98%)\n",
      "Test set: Accuracy: 1376/1400 (98%)\n",
      "Test set: Accuracy: 1425/1450 (98%)\n",
      "Test set: Accuracy: 1475/1500 (98%)\n",
      "Test set: Accuracy: 1524/1550 (98%)\n",
      "Test set: Accuracy: 1571/1600 (98%)\n",
      "Test set: Accuracy: 1620/1650 (98%)\n",
      "Test set: Accuracy: 1669/1700 (98%)\n",
      "Test set: Accuracy: 1718/1750 (98%)\n",
      "Test set: Accuracy: 1768/1800 (98%)\n",
      "Test set: Accuracy: 1817/1850 (98%)\n",
      "Test set: Accuracy: 1866/1900 (98%)\n",
      "Test set: Accuracy: 1916/1950 (98%)\n",
      "Test set: Accuracy: 1965/2000 (98%)\n",
      "Test set: Accuracy: 2014/2050 (98%)\n",
      "Test set: Accuracy: 2062/2100 (98%)\n",
      "Test set: Accuracy: 2112/2150 (98%)\n",
      "Test set: Accuracy: 2161/2200 (98%)\n",
      "Test set: Accuracy: 2211/2250 (98%)\n",
      "Test set: Accuracy: 2259/2300 (98%)\n",
      "Test set: Accuracy: 2309/2350 (98%)\n",
      "Test set: Accuracy: 2359/2400 (98%)\n",
      "Test set: Accuracy: 2409/2450 (98%)\n",
      "Test set: Accuracy: 2457/2500 (98%)\n",
      "Test set: Accuracy: 2506/2550 (98%)\n",
      "Test set: Accuracy: 2555/2600 (98%)\n",
      "Test set: Accuracy: 2605/2650 (98%)\n",
      "Test set: Accuracy: 2654/2700 (98%)\n",
      "Test set: Accuracy: 2702/2750 (98%)\n",
      "Test set: Accuracy: 2751/2800 (98%)\n",
      "Test set: Accuracy: 2801/2850 (98%)\n",
      "Test set: Accuracy: 2851/2900 (98%)\n",
      "Test set: Accuracy: 2900/2950 (98%)\n",
      "Test set: Accuracy: 2948/3000 (98%)\n",
      "Test set: Accuracy: 2998/3050 (98%)\n",
      "Test set: Accuracy: 3046/3100 (98%)\n",
      "Test set: Accuracy: 3094/3150 (98%)\n",
      "Test set: Accuracy: 3141/3200 (98%)\n",
      "Test set: Accuracy: 3189/3250 (98%)\n",
      "Test set: Accuracy: 3239/3300 (98%)\n",
      "Test set: Accuracy: 3289/3350 (98%)\n",
      "Test set: Accuracy: 3339/3400 (98%)\n",
      "Test set: Accuracy: 3388/3450 (98%)\n",
      "Test set: Accuracy: 3435/3500 (98%)\n",
      "Test set: Accuracy: 3485/3550 (98%)\n",
      "Test set: Accuracy: 3535/3600 (98%)\n",
      "Test set: Accuracy: 3583/3650 (98%)\n",
      "Test set: Accuracy: 3632/3700 (98%)\n",
      "Test set: Accuracy: 3682/3750 (98%)\n",
      "Test set: Accuracy: 3729/3800 (98%)\n",
      "Test set: Accuracy: 3778/3850 (98%)\n",
      "Test set: Accuracy: 3828/3900 (98%)\n",
      "Test set: Accuracy: 3878/3950 (98%)\n",
      "Test set: Accuracy: 3928/4000 (98%)\n",
      "Test set: Accuracy: 3978/4050 (98%)\n",
      "Test set: Accuracy: 4028/4100 (98%)\n",
      "Test set: Accuracy: 4077/4150 (98%)\n",
      "Test set: Accuracy: 4127/4200 (98%)\n",
      "Test set: Accuracy: 4177/4250 (98%)\n",
      "Test set: Accuracy: 4226/4300 (98%)\n",
      "Test set: Accuracy: 4273/4350 (98%)\n",
      "Test set: Accuracy: 4322/4400 (98%)\n",
      "Test set: Accuracy: 4372/4450 (98%)\n",
      "Test set: Accuracy: 4421/4500 (98%)\n",
      "Test set: Accuracy: 4471/4550 (98%)\n",
      "Test set: Accuracy: 4520/4600 (98%)\n",
      "Test set: Accuracy: 4570/4650 (98%)\n",
      "Test set: Accuracy: 4620/4700 (98%)\n",
      "Test set: Accuracy: 4670/4750 (98%)\n",
      "Test set: Accuracy: 4719/4800 (98%)\n",
      "Test set: Accuracy: 4767/4850 (98%)\n",
      "Test set: Accuracy: 4817/4900 (98%)\n",
      "Test set: Accuracy: 4866/4950 (98%)\n",
      "Test set: Accuracy: 4916/5000 (98%)\n",
      "Test set: Accuracy: 4966/5050 (98%)\n",
      "Test set: Accuracy: 5015/5100 (98%)\n",
      "Test set: Accuracy: 5063/5150 (98%)\n",
      "Test set: Accuracy: 5113/5200 (98%)\n",
      "Test set: Accuracy: 5163/5250 (98%)\n",
      "Test set: Accuracy: 5213/5300 (98%)\n",
      "Test set: Accuracy: 5263/5350 (98%)\n",
      "Test set: Accuracy: 5313/5400 (98%)\n",
      "Test set: Accuracy: 5362/5450 (98%)\n",
      "Test set: Accuracy: 5412/5500 (98%)\n",
      "Test set: Accuracy: 5460/5550 (98%)\n",
      "Test set: Accuracy: 5509/5600 (98%)\n",
      "Test set: Accuracy: 5558/5650 (98%)\n",
      "Test set: Accuracy: 5608/5700 (98%)\n",
      "Test set: Accuracy: 5656/5750 (98%)\n",
      "Test set: Accuracy: 5705/5800 (98%)\n",
      "Test set: Accuracy: 5754/5850 (98%)\n",
      "Test set: Accuracy: 5804/5900 (98%)\n",
      "Test set: Accuracy: 5851/5950 (98%)\n",
      "Test set: Accuracy: 5900/6000 (98%)\n",
      "Test set: Accuracy: 5950/6050 (98%)\n",
      "Test set: Accuracy: 5999/6100 (98%)\n",
      "Test set: Accuracy: 6048/6150 (98%)\n",
      "Test set: Accuracy: 6097/6200 (98%)\n",
      "Test set: Accuracy: 6147/6250 (98%)\n",
      "Test set: Accuracy: 6197/6300 (98%)\n",
      "Test set: Accuracy: 6246/6350 (98%)\n",
      "Test set: Accuracy: 6295/6400 (98%)\n",
      "Test set: Accuracy: 6343/6450 (98%)\n",
      "Test set: Accuracy: 6392/6500 (98%)\n",
      "Test set: Accuracy: 6441/6550 (98%)\n",
      "Test set: Accuracy: 6489/6600 (98%)\n",
      "Test set: Accuracy: 6539/6650 (98%)\n",
      "Test set: Accuracy: 6588/6700 (98%)\n",
      "Test set: Accuracy: 6638/6750 (98%)\n",
      "Test set: Accuracy: 6688/6800 (98%)\n",
      "Test set: Accuracy: 6735/6850 (98%)\n",
      "Test set: Accuracy: 6784/6900 (98%)\n",
      "Test set: Accuracy: 6831/6950 (98%)\n",
      "Test set: Accuracy: 6879/7000 (98%)\n",
      "Test set: Accuracy: 6929/7050 (98%)\n",
      "Test set: Accuracy: 6979/7100 (98%)\n",
      "Test set: Accuracy: 7027/7150 (98%)\n",
      "Test set: Accuracy: 7076/7200 (98%)\n",
      "Test set: Accuracy: 7126/7250 (98%)\n",
      "Test set: Accuracy: 7174/7300 (98%)\n",
      "Test set: Accuracy: 7223/7350 (98%)\n",
      "Test set: Accuracy: 7273/7400 (98%)\n",
      "Test set: Accuracy: 7323/7450 (98%)\n",
      "Test set: Accuracy: 7373/7500 (98%)\n",
      "Test set: Accuracy: 7423/7550 (98%)\n",
      "Test set: Accuracy: 7471/7600 (98%)\n",
      "Test set: Accuracy: 7520/7650 (98%)\n",
      "Test set: Accuracy: 7570/7700 (98%)\n",
      "Test set: Accuracy: 7618/7750 (98%)\n",
      "Test set: Accuracy: 7667/7800 (98%)\n",
      "Test set: Accuracy: 7717/7850 (98%)\n",
      "Test set: Accuracy: 7763/7900 (98%)\n",
      "Test set: Accuracy: 7811/7950 (98%)\n",
      "Test set: Accuracy: 7861/8000 (98%)\n",
      "Test set: Accuracy: 7911/8050 (98%)\n",
      "Test set: Accuracy: 7961/8100 (98%)\n",
      "Test set: Accuracy: 8011/8150 (98%)\n",
      "Test set: Accuracy: 8058/8200 (98%)\n",
      "Test set: Accuracy: 8107/8250 (98%)\n",
      "Test set: Accuracy: 8156/8300 (98%)\n",
      "Test set: Accuracy: 8206/8350 (98%)\n",
      "Test set: Accuracy: 8255/8400 (98%)\n",
      "Test set: Accuracy: 8305/8450 (98%)\n",
      "Test set: Accuracy: 8355/8500 (98%)\n",
      "Test set: Accuracy: 8404/8550 (98%)\n",
      "Test set: Accuracy: 8454/8600 (98%)\n",
      "Test set: Accuracy: 8504/8650 (98%)\n",
      "Test set: Accuracy: 8553/8700 (98%)\n",
      "Test set: Accuracy: 8602/8750 (98%)\n",
      "Test set: Accuracy: 8652/8800 (98%)\n",
      "Test set: Accuracy: 8702/8850 (98%)\n",
      "Test set: Accuracy: 8751/8900 (98%)\n",
      "Test set: Accuracy: 8800/8950 (98%)\n",
      "Test set: Accuracy: 8850/9000 (98%)\n",
      "Test set: Accuracy: 8900/9050 (98%)\n",
      "Test set: Accuracy: 8946/9100 (98%)\n",
      "Test set: Accuracy: 8996/9150 (98%)\n",
      "Test set: Accuracy: 9044/9200 (98%)\n",
      "Test set: Accuracy: 9093/9250 (98%)\n",
      "Test set: Accuracy: 9142/9300 (98%)\n",
      "Test set: Accuracy: 9192/9350 (98%)\n",
      "Test set: Accuracy: 9240/9400 (98%)\n",
      "Test set: Accuracy: 9290/9450 (98%)\n",
      "Test set: Accuracy: 9340/9500 (98%)\n",
      "Test set: Accuracy: 9387/9550 (98%)\n",
      "Test set: Accuracy: 9436/9600 (98%)\n",
      "Test set: Accuracy: 9485/9650 (98%)\n",
      "Test set: Accuracy: 9533/9700 (98%)\n",
      "Test set: Accuracy: 9581/9750 (98%)\n",
      "Test set: Accuracy: 9631/9800 (98%)\n",
      "Test set: Accuracy: 9680/9850 (98%)\n",
      "Test set: Accuracy: 9728/9900 (98%)\n",
      "Test set: Accuracy: 9777/9950 (98%)\n",
      "Test set: Accuracy: 9826/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Here you are, you have learned how to do end to end secure predictions: the weights of the server's model have not leaked to the client and the server has no information about the data input nor the classification output!\n",
    "\n",
    "Regarding performance, classifying one image takes **less than 0.1 second**, approximately **33ms** on my laptop (2,7 GHz Intel Core i7, 16GB RAM). However, this is using very fast communication (all the workers are on my local machine). Performance will vary depending on how fast different workers can talk to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have seen how easy it is to leverage PyTorch and PySyft to perform practical Secure Machine Learning and protect users data, without having to be a crypto expert!\n",
    "\n",
    "More on this topic will come soon, including convolutional layers to properly benchmark PySyft performance with respect to other libraries, as well as private encrypted training of neural networks, which is needed when a organisation resorts to external sensitive data to train its own model. Stay tuned!\n",
    "\n",
    "If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways! \n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Pick our tutorials on GitHub!\n",
    "\n",
    "We made really nice tutorials to get a better understanding of what Federated and Privacy-Preserving Learning should look like and how we are building the bricks for this to happen.\n",
    "\n",
    "- [Checkout the PySyft tutorials](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! If you want to start \"one off\" mini-projects, you can go to PySyft GitHub Issues page and search for issues marked `Good First Issue`.\n",
    "\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "- [Donate through OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
